# Projet Big Data - Traitement DistribuÃ©
**Master 1 UCAO 2024-2025**

## ğŸ¯ Objectif
DÃ©ployer un environnement Big Data complet avec Hadoop, Spark, MongoDB et Apache Pig pour l'analyse de donnÃ©es distribuÃ©e.

## ğŸ—ï¸ Architecture du SystÃ¨me

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   NameNode      â”‚    â”‚  Secondary NN   â”‚    â”‚   DataNodes     â”‚
â”‚  (Master)       â”‚    â”‚                 â”‚    â”‚   (3 Slaves)    â”‚
â”‚                 â”‚    â”‚                 â”‚    â”‚                 â”‚
â”‚ - Hadoop Master â”‚    â”‚ - Backup        â”‚    â”‚ - Data Storage  â”‚
â”‚ - Spark Master  â”‚    â”‚ - Checkpoints   â”‚    â”‚ - Processing    â”‚
â”‚ - Pig Runtime   â”‚    â”‚                 â”‚    â”‚ - Spark Workers â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚                       â”‚                       â”‚
         â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                 â”‚
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚    MongoDB      â”‚
                    â”‚   (Database)    â”‚
                    â”‚                 â”‚
                    â”‚ - Data Storage  â”‚
                    â”‚ - Collections   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸš€ Installation Rapide sur Ubuntu

### 1. PrÃ©requis
```bash
# Cloner le projet
git clone <your-repo>
cd traitement-distribue-2024-2025

# Installer Docker et Docker Compose
chmod +x setup.sh
./setup.sh
```

### 2. Lancer l'environnement
```bash
# DÃ©marrer tous les services
docker-compose up -d

# VÃ©rifier que tous les services sont actifs
docker-compose ps
```

### 3. ExÃ©cuter le pipeline complet
```bash
chmod +x run-project.sh
./run-project.sh
```

## ğŸ“Š Composants du Projet

### 1. Cluster Hadoop (5 nÅ“uds)
- **NameNode** : NÅ“ud maÃ®tre (namenode:9870)
- **Secondary NameNode** : NÅ“ud secondaire (secondary-namenode:9868)
- **DataNodes** : 3 nÅ“uds esclaves (datanode1-3:9864-9866)

### 2. Apache Spark
- **Master** : Coordination des jobs (namenode:8080)
- **Workers** : Traitement distribuÃ© sur les DataNodes

### 3. Apache Pig
- **Scripts d'analyse** : Exploration des donnÃ©es
- **IntÃ©gration MongoDB** : Lecture/Ã©criture de donnÃ©es

### 4. MongoDB
- **Base de donnÃ©es** : Stockage NoSQL (mongodb:27017)
- **Collections** : employees, results

### 5. Application Dynamique
- **Dashboard Web** : Interface de monitoring (localhost:5000)
- **Streaming** : Traitement en temps rÃ©el avec Spark Streaming

## ğŸ“‹ Ã‰tapes d'ExÃ©cution

### Ã‰tape 1: Analyse avec Apache Pig
```bash
# ExÃ©cuter l'analyse exploratoire
docker exec namenode pig -f /scripts/pig/data-exploration.pig

# RÃ©sultats disponibles dans HDFS
docker exec namenode hdfs dfs -ls /data/output/
```

### Ã‰tape 2: IntÃ©gration MongoDB
```bash
# Test de connexion MongoDB-Hadoop
docker exec namenode pig -f /scripts/pig/mongodb-connection.pig

# VÃ©rifier les donnÃ©es MongoDB
docker exec mongodb mongo bigdata --eval "db.employees.find().pretty()"
```

### Ã‰tape 3: Application de Streaming
```bash
# AccÃ©der au dashboard
open http://localhost:5000

# DÃ©marrer le traitement en temps rÃ©el
curl http://localhost:5000/api/start
```

## ğŸ” Interfaces Web

| Service | URL | Description |
|---------|-----|-------------|
| Hadoop NameNode | http://localhost:9870 | Interface HDFS |
| Spark Master | http://localhost:8080 | Cluster Spark |
| Dashboard Streaming | http://localhost:5000 | Application dynamique |

## ğŸ“ˆ RÃ©sultats Attendus

### 1. Analyse Pig - Statistiques par DÃ©partement
```
IT,5,52600.0,41000.0,68000.0
Sales,3,56333.33,52000.0,61000.0
Finance,2,62500.0,57000.0,68000.0
```

### 2. Distribution par Ville
```
Paris,2
London,1
Tokyo,1
...
```

### 3. Groupes d'Ã‚ge
```
Young,8,45250.0
Middle,7,58571.43
Senior,5,61800.0
```

## ğŸ¬ VidÃ©o de DÃ©monstration

### Plan de la VidÃ©o (10-15 minutes)
1. **Introduction** (1 min)
   - PrÃ©sentation du projet
   - Architecture du systÃ¨me

2. **Installation** (2-3 min)
   - Docker Compose up
   - VÃ©rification des services

3. **Hadoop & HDFS** (2-3 min)
   - Interface web NameNode
   - Chargement des donnÃ©es
   - Navigation dans HDFS

4. **Apache Pig** (3-4 min)
   - ExÃ©cution des scripts
   - Analyse exploratoire
   - RÃ©sultats dans HDFS

5. **MongoDB Integration** (2-3 min)
   - Connexion Hadoop-MongoDB
   - Lecture des donnÃ©es
   - Stockage des rÃ©sultats

6. **Application Dynamique** (2-3 min)
   - Dashboard web
   - Streaming en temps rÃ©el
   - Visualisations interactives

7. **Conclusion** (1 min)
   - RÃ©capitulatif
   - Workflow final

## ğŸ“ Structure des Fichiers

```
traitement-distribue-2024-2025/
â”œâ”€â”€ docker-compose.yml              # Orchestration
â”œâ”€â”€ docker/                         # Dockerfiles
â”œâ”€â”€ scripts/pig/                     # Scripts Pig
â”œâ”€â”€ applications/streaming-app/      # App dynamique
â”œâ”€â”€ data/                           # DonnÃ©es d'exemple
â”œâ”€â”€ setup.sh                       # Installation
â”œâ”€â”€ run-project.sh                  # ExÃ©cution complÃ¨te
â””â”€â”€ README.md                       # Documentation
```

## ğŸ› ï¸ Commandes Utiles

```bash
# Status des conteneurs
docker-compose ps

# Logs d'un service
docker-compose logs namenode

# Shell dans un conteneur
docker exec -it namenode bash

# ArrÃªter tous les services
docker-compose down

# Nettoyer complÃ¨tement
docker-compose down -v --remove-orphans
```

## ğŸ”§ DÃ©pannage

### ProblÃ¨me : Services ne dÃ©marrent pas
```bash
# VÃ©rifier les logs
docker-compose logs

# RedÃ©marrer un service spÃ©cifique
docker-compose restart namenode
```

### ProblÃ¨me : Ports occupÃ©s
```bash
# Changer les ports dans docker-compose.yml
# Ou arrÃªter les services qui utilisent ces ports
sudo lsof -i :9870
```

## ğŸ“ Livrables

âœ… **Dockerfiles et scripts**
âœ… **Scripts Pig pour l'analyse**  
âœ… **Configuration Hadoop-MongoDB**
âœ… **Application dynamique**
âœ… **Diagrammes de workflow**
âœ… **VidÃ©o de prÃ©sentation**

## ğŸ‘¨â€ğŸ’» Auteur
**Ã‰tudiant Master 1 UCAO**  
**AnnÃ©e acadÃ©mique 2024-2025**

---

*Date limite de rendu : Vendredi 29 aoÃ»t 2025*