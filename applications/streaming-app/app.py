#!/usr/bin/env python3
"""
Real Spark Streaming Application for Big Data Project
Using actual Spark jobs to process CSV data
"""

from flask import Flask, render_template, jsonify
import os
import sys
import time
import threading
import json
import random
import subprocess

# Configuration Spark
os.environ['SPARK_HOME'] = '/opt/spark'
os.environ['PYTHONPATH'] = '/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip'
sys.path.append('/opt/spark/python')
sys.path.append('/opt/spark/python/lib/py4j-0.10.9.7-src.zip')

try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    SPARK_AVAILABLE = True
    print("‚úÖ Spark imports r√©ussis")
except Exception as e:
    print(f"‚ö†Ô∏è Spark import √©chou√©: {e}")
    SPARK_AVAILABLE = False

app = Flask(__name__)

# Variables globales pour les r√©sultats
streaming_results = {
    'total_records': 0,
    'avg_salary': 0,
    'department_counts': {},
    'last_update': None,
    'processing_rate': 0,
    'spark_jobs_count': 0,
    'real_csv_records': 0,
    'spark_status': 'Not Started'
}

class RealSparkProcessor:
    def __init__(self):
        self.spark = None
        self.original_data = None
        self.streaming_active = False
        
    def initialize_spark(self):
        """Initialiser une vraie session Spark"""
        try:
            self.spark = SparkSession.builder \
                .appName("BigDataRealStreaming") \
                .master("spark://namenode:7077") \
                .config("spark.executor.memory", "1g") \
                .config("spark.driver.memory", "1g") \
                .config("spark.executor.cores", "2") \
                .config("spark.default.parallelism", "6") \
                .getOrCreate()
            
            self.spark.sparkContext.setLogLevel("INFO")  # Pour voir les jobs
            print("‚úÖ Session Spark cr√©√©e avec Master: spark://namenode:7077")
            return True
        except Exception as e:
            print(f"‚ùå Erreur Spark Session: {e}")
            return False
    
    def load_real_csv_data(self):
        """Charger les vraies donn√©es CSV avec Spark"""
        global streaming_results
        
        if not self.spark:
            if not self.initialize_spark():
                return False
        
        try:
            # D√©finir le sch√©ma exact de vos donn√©es
            schema = StructType([
                StructField("id", IntegerType(), True),
                StructField("name", StringType(), True), 
                StructField("age", IntegerType(), True),
                StructField("city", StringType(), True),
                StructField("salary", DoubleType(), True),
                StructField("department", StringType(), True)
            ])
            
            # Lire depuis HDFS avec Spark
            hdfs_path = "hdfs://namenode:9000/data/input/sample_data.csv"
            
            df = self.spark.read \
                .option("header", "false") \
                .schema(schema) \
                .csv(hdfs_path)
            
            # Cacher les donn√©es pour r√©utilisation
            df.cache()
            self.original_data = df
            
            # Calculer les statistiques r√©elles avec Spark
            total_count = df.count()
            avg_salary = df.agg(avg("salary")).collect()[0][0]
            
            # Groupement par d√©partement avec Spark
            dept_df = df.groupBy("department").count().orderBy("count", ascending=False)
            dept_results = dept_df.collect()
            dept_counts = {row["department"]: row["count"] for row in dept_results}
            
            # Mettre √† jour les r√©sultats
            streaming_results.update({
                'real_csv_records': total_count,
                'total_records': total_count,
                'avg_salary': round(avg_salary, 2) if avg_salary else 0,
                'department_counts': dept_counts,
                'last_update': time.strftime('%Y-%m-%d %H:%M:%S'),
                'spark_status': 'Data Loaded',
                'spark_jobs_count': 1
            })
            
            print(f"‚úÖ Donn√©es charg√©es avec Spark: {total_count} enregistrements")
            print(f"üìä D√©partements trouv√©s: {list(dept_counts.keys())}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur chargement Spark: {e}")
            return False
    
    def start_real_spark_streaming(self):
        """D√©marrer le vrai streaming Spark"""
        global streaming_results
        
        if not self.original_data:
            print("‚ùå Pas de donn√©es originales")
            return
        
        self.streaming_active = True
        streaming_results['spark_status'] = 'Streaming Active'
        job_counter = 1
        
        print("üîÑ D√©marrage du Spark Streaming r√©el...")
        
        while self.streaming_active:
            try:
                # Cr√©er des micro-batches en √©chantillonnant les donn√©es
                sample_fraction = random.uniform(0.3, 0.8)
                batch_df = self.original_data.sample(sample_fraction, seed=int(time.time()))
                
                print(f"üìä Traitement Spark Job #{job_counter}")
                
                # Traitement Spark distribu√©
                batch_count = batch_df.count()
                batch_avg_salary = batch_df.agg(avg("salary")).collect()[0][0]
                
                # Analyse par d√©partement pour ce batch
                dept_analysis = batch_df.groupBy("department") \
                    .agg(
                        count("*").alias("count"),
                        avg("salary").alias("avg_salary"),
                        min("salary").alias("min_salary"),
                        max("salary").alias("max_salary")
                    ).collect()
                
                # Analyse par tranche d'√¢ge
                age_analysis = batch_df.withColumn(
                    "age_group", 
                    when(col("age") < 30, "Young")
                    .when(col("age") < 50, "Middle")
                    .otherwise("Senior")
                ).groupBy("age_group").count().collect()
                
                # Mettre √† jour les statistiques globales
                streaming_results['total_records'] += batch_count
                streaming_results['avg_salary'] = round(batch_avg_salary, 2) if batch_avg_salary else 0
                streaming_results['processing_rate'] = batch_count
                streaming_results['spark_jobs_count'] = job_counter
                streaming_results['last_update'] = time.strftime('%Y-%m-%d %H:%M:%S')
                
                # Mettre √† jour les d√©partements avec les vrais r√©sultats Spark
                new_dept_counts = {}
                for row in dept_analysis:
                    new_dept_counts[row["department"]] = row["count"]
                streaming_results['department_counts'] = new_dept_counts
                
                print(f"‚úÖ Job #{job_counter} termin√©: {batch_count} records, avg salary: {batch_avg_salary:.2f}")
                print(f"üìà Total trait√©: {streaming_results['total_records']} records")
                
                job_counter += 1
                time.sleep(10)  # Intervalle entre les jobs
                
            except Exception as e:
                print(f"‚ùå Erreur Spark Streaming: {e}")
                time.sleep(15)
    
    def stop_streaming(self):
        """Arr√™ter le streaming"""
        self.streaming_active = False
        streaming_results['spark_status'] = 'Stopped'
    
    def get_spark_application_info(self):
        """R√©cup√©rer les infos sur les applications Spark"""
        try:
            if self.spark:
                app_id = self.spark.sparkContext.applicationId
                app_name = self.spark.sparkContext.appName
                return {
                    'application_id': app_id,
                    'application_name': app_name,
                    'master_url': 'spark://namenode:7077',
                    'executor_count': len(self.spark.sparkContext.statusTracker().getExecutorInfos()) - 1
                }
        except:
            pass
        return None

# Instance globale
spark_processor = RealSparkProcessor()

@app.route('/')
def dashboard():
    """Dashboard principal"""
    return render_template('dashboard.html')

@app.route('/api/stats')
def get_stats():
    """API pour r√©cup√©rer les statistiques"""
    # Ajouter les infos Spark
    spark_info = spark_processor.get_spark_application_info()
    result = streaming_results.copy()
    if spark_info:
        result['spark_application'] = spark_info
    return jsonify(result)

@app.route('/api/start')
def start_processing():
    """D√©marrer le traitement Spark r√©el"""
    try:
        # Charger les donn√©es avec Spark
        if spark_processor.load_real_csv_data():
            # D√©marrer le streaming Spark en arri√®re-plan
            thread = threading.Thread(target=spark_processor.start_real_spark_streaming, daemon=True)
            thread.start()
            
            return jsonify({
                'status': 'started',
                'message': f'Spark Streaming d√©marr√© avec {streaming_results["real_csv_records"]} records r√©els',
                'spark_master': 'spark://namenode:7077',
                'data_source': 'HDFS via Spark'
            })
        else:
            return jsonify({'status': 'error', 'message': '√âchec chargement donn√©es Spark'})
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'Erreur Spark: {str(e)}'})

@app.route('/api/stop')
def stop_processing():
    """Arr√™ter le streaming"""
    spark_processor.stop_streaming()
    return jsonify({'status': 'stopped', 'message': 'Spark Streaming arr√™t√©'})

@app.route('/api/spark-info')
def get_spark_info():
    """Informations d√©taill√©es sur Spark"""
    info = spark_processor.get_spark_application_info()
    if info:
        return jsonify(info)
    else:
        return jsonify({'error': 'Pas de session Spark active'})

@app.route('/api/real-data')
def get_real_data():
    """Afficher un √©chantillon des vraies donn√©es CSV"""
    try:
        if spark_processor.original_data:
            sample_data = spark_processor.original_data.limit(10).collect()
            result = [row.asDict() for row in sample_data]
            return jsonify({
                'status': 'success',
                'total_records': streaming_results['real_csv_records'],
                'sample_data': result
            })
        else:
            return jsonify({'error': 'Pas de donn√©es Spark charg√©es'})
    except Exception as e:
        return jsonify({'error': str(e)})

if __name__ == '__main__':
    print("üöÄ D√©marrage Application Spark Streaming R√âELLE")
    print("‚ö° Connexion √† Spark Master: spark://namenode:7077")
    
    # D√©marrer Flask
    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)