#!/usr/bin/env python3
"""
Real Spark Streaming Application for Big Data Project
Using actual Spark jobs to process CSV data - FIXED VERSION
"""

from flask import Flask, render_template, jsonify
import os
import sys
import time
import threading
import json
import random
import subprocess

# Configuration Spark
os.environ['SPARK_HOME'] = '/opt/spark'
os.environ['PYTHONPATH'] = '/opt/spark/python:/opt/spark/python/lib/py4j-0.10.9.7-src.zip'
sys.path.append('/opt/spark/python')
sys.path.append('/opt/spark/python/lib/py4j-0.10.9.7-src.zip')

try:
    from pyspark.sql import SparkSession
    from pyspark.sql.functions import *
    from pyspark.sql.types import *
    SPARK_AVAILABLE = True
    print("‚úÖ Spark imports r√©ussis")
except Exception as e:
    print(f"‚ö†Ô∏è Spark import √©chou√©: {e}")
    SPARK_AVAILABLE = False

app = Flask(__name__)

# Variables globales pour les r√©sultats
streaming_results = {
    'total_records': 0,
    'avg_salary': 0,
    'department_counts': {},
    'last_update': None,
    'processing_rate': 0,
    'spark_jobs_count': 0,
    'real_csv_records': 10,  # Fix√© √† 10 car vous avez 10 lignes
    'spark_status': 'Not Started'
}

class RealSparkProcessor:
    def __init__(self):
        self.spark = None
        self.original_data = None
        self.streaming_active = False
        self.data_processed_count = 0
        
    def initialize_spark(self):
        """Initialiser une vraie session Spark"""
        try:
            self.spark = SparkSession.builder \
                .appName("BigDataRealStreaming") \
                .master("spark://namenode:7077") \
                .config("spark.executor.memory", "1g") \
                .config("spark.driver.memory", "1g") \
                .config("spark.executor.cores", "2") \
                .config("spark.default.parallelism", "6") \
                .getOrCreate()
            
            self.spark.sparkContext.setLogLevel("INFO")
            print("‚úÖ Session Spark cr√©√©e avec Master: spark://namenode:7077")
            return True
        except Exception as e:
            print(f"‚ùå Erreur Spark Session: {e}")
            return False
    
    def load_real_csv_data(self):
        """Charger les vraies donn√©es CSV avec Spark - UNIQUEMENT VOS 10 LIGNES"""
        global streaming_results
        
        if not self.spark:
            if not self.initialize_spark():
                return False
        
        try:
            # D√©finir le sch√©ma exact de vos donn√©es
            schema = StructType([
                StructField("id", IntegerType(), True),
                StructField("name", StringType(), True), 
                StructField("age", IntegerType(), True),
                StructField("city", StringType(), True),
                StructField("salary", DoubleType(), True),
                StructField("department", StringType(), True)
            ])
            
            # Lire depuis HDFS avec Spark
            hdfs_path = "hdfs://namenode:9000/data/input/sample_data.csv"
            
            df = self.spark.read \
                .option("header", "false") \
                .schema(schema) \
                .csv(hdfs_path)
            
            # Cacher les donn√©es pour r√©utilisation
            df.cache()
            self.original_data = df
            
            # Calculer les statistiques r√©elles avec Spark - UNE SEULE FOIS
            total_count = df.count()
            avg_salary = df.agg(avg("salary")).collect()[0][0]
            
            # Groupement par d√©partement avec Spark
            dept_df = df.groupBy("department").count().orderBy("count", ascending=False)
            dept_results = dept_df.collect()
            dept_counts = {row["department"]: row["count"] for row in dept_results}
            
            # Mettre √† jour les r√©sultats FIXES
            streaming_results.update({
                'real_csv_records': total_count,
                'total_records': total_count,
                'avg_salary': round(avg_salary, 2) if avg_salary else 0,
                'department_counts': dept_counts,
                'last_update': time.strftime('%Y-%m-%d %H:%M:%S'),
                'spark_status': 'Data Loaded - Static Analysis',
                'spark_jobs_count': 1,
                'processing_rate': total_count
            })
            
            print(f"‚úÖ Donn√©es charg√©es avec Spark: {total_count} enregistrements")
            print(f"üìä D√©partements trouv√©s: {list(dept_counts.keys())}")
            print(f"üí∞ Salaire moyen: {avg_salary:.2f}")
            
            return True
            
        except Exception as e:
            print(f"‚ùå Erreur chargement Spark: {e}")
            return False
    
    def start_real_spark_streaming(self):
        """D√©marrer l'analyse statique des donn√©es r√©elles - PAS DE G√âN√âRATION"""
        global streaming_results
        
        if not self.original_data:
            print("‚ùå Pas de donn√©es originales")
            return
        
        self.streaming_active = True
        streaming_results['spark_status'] = 'Analyzing Real Data'
        job_counter = 1
        
        print("üîÑ D√©marrage de l'analyse Spark des donn√©es r√©elles...")
        
        # Analyse compl√®te UNE SEULE FOIS
        try:
            print(f"üìä Analyse Spark Job #{job_counter} - VOS DONN√âES R√âELLES")
            
            # Traitement Spark distribu√© de VOS vraies donn√©es
            total_records = self.original_data.count()
            avg_salary_result = self.original_data.agg(avg("salary")).collect()[0][0]
            
            # Analyse par d√©partement pour VOS donn√©es
            dept_analysis = self.original_data.groupBy("department") \
                .agg(
                    count("*").alias("count"),
                    avg("salary").alias("avg_salary"),
                    min("salary").alias("min_salary"),
                    max("salary").alias("max_salary")
                ).collect()
            
            # Analyse par ville pour VOS donn√©es
            city_analysis = self.original_data.groupBy("city") \
                .agg(count("*").alias("count")) \
                .orderBy("count", ascending=False).collect()
            
            # Analyse par tranche d'√¢ge pour VOS donn√©es
            age_analysis = self.original_data.withColumn(
                "age_group", 
                when(col("age") < 30, "Young")
                .when(col("age") < 50, "Middle")
                .otherwise("Senior")
            ).groupBy("age_group") \
             .agg(
                 count("*").alias("count"),
                 avg("salary").alias("avg_salary")
             ).collect()
            
            # Afficher les r√©sultats r√©els
            print("üìä R√âSULTATS DE VOS VRAIES DONN√âES:")
            print(f"   Total employ√©s: {total_records}")
            print(f"   Salaire moyen: {avg_salary_result:.2f}")
            
            print("üìä Analyse par d√©partement:")
            dept_counts = {}
            for row in dept_analysis:
                dept_name = row["department"]
                dept_count = row["count"]
                dept_avg_salary = row["avg_salary"]
                dept_counts[dept_name] = dept_count
                print(f"   {dept_name}: {dept_count} employ√©s, salaire moyen: {dept_avg_salary:.2f}")
            
            print("üìä Analyse par ville:")
            for row in city_analysis:
                print(f"   {row['city']}: {row['count']} employ√©s")
                
            print("üìä Analyse par tranche d'√¢ge:")
            for row in age_analysis:
                print(f"   {row['age_group']}: {row['count']} employ√©s, salaire moyen: {row['avg_salary']:.2f}")
            
            # Mettre √† jour les statistiques FINALES avec VOS vraies donn√©es
            streaming_results.update({
                'total_records': total_records,
                'avg_salary': round(avg_salary_result, 2) if avg_salary_result else 0,
                'processing_rate': total_records,
                'spark_jobs_count': job_counter,
                'last_update': time.strftime('%Y-%m-%d %H:%M:%S'),
                'department_counts': dept_counts,
                'spark_status': f'Analysis Complete - {total_records} Real Records Processed'
            })
            
            print(f"‚úÖ Analyse termin√©e: {total_records} vrais enregistrements trait√©s")
            
            # Arr√™ter le streaming car l'analyse est termin√©e
            self.streaming_active = False
            streaming_results['spark_status'] = 'Analysis Complete - Real Data Only'
            
        except Exception as e:
            print(f"‚ùå Erreur Spark Streaming: {e}")
            self.streaming_active = False
            streaming_results['spark_status'] = 'Analysis Failed'
    
    def stop_streaming(self):
        """Arr√™ter le streaming"""
        self.streaming_active = False
        streaming_results['spark_status'] = 'Stopped'
    
    def get_spark_application_info(self):
        """R√©cup√©rer les infos sur les applications Spark"""
        try:
            if self.spark:
                app_id = self.spark.sparkContext.applicationId
                app_name = self.spark.sparkContext.appName
                return {
                    'application_id': app_id,
                    'application_name': app_name,
                    'master_url': 'spark://namenode:7077',
                    'executor_count': len(self.spark.sparkContext.statusTracker().getExecutorInfos()) - 1,
                    'data_source': 'Real CSV Data (10 records)',
                    'analysis_type': 'Static Analysis of Real Data'
                }
        except:
            pass
        return None

# Instance globale
spark_processor = RealSparkProcessor()

@app.route('/')
def dashboard():
    """Dashboard principal"""
    return render_template('dashboard.html')

@app.route('/api/stats')
def get_stats():
    """API pour r√©cup√©rer les statistiques"""
    # Ajouter les infos Spark
    spark_info = spark_processor.get_spark_application_info()
    result = streaming_results.copy()
    if spark_info:
        result['spark_application'] = spark_info
    return jsonify(result)

@app.route('/api/start')
def start_processing():
    """D√©marrer le traitement Spark r√©el"""
    try:
        # Charger les donn√©es avec Spark
        if spark_processor.load_real_csv_data():
            # D√©marrer l'analyse Spark en arri√®re-plan
            thread = threading.Thread(target=spark_processor.start_real_spark_streaming, daemon=True)
            thread.start()
            
            return jsonify({
                'status': 'started',
                'message': f'Analyse Spark d√©marr√©e avec {streaming_results["real_csv_records"]} vrais enregistrements',
                'spark_master': 'spark://namenode:7077',
                'data_source': 'HDFS - Vos vraies donn√©es CSV (10 lignes)',
                'analysis_type': 'Static Analysis - No Data Generation'
            })
        else:
            return jsonify({'status': 'error', 'message': '√âchec chargement donn√©es Spark'})
    except Exception as e:
        return jsonify({'status': 'error', 'message': f'Erreur Spark: {str(e)}'})

@app.route('/api/stop')
def stop_processing():
    """Arr√™ter le streaming"""
    spark_processor.stop_streaming()
    return jsonify({'status': 'stopped', 'message': 'Analyse Spark arr√™t√©e'})

@app.route('/api/spark-info')
def get_spark_info():
    """Informations d√©taill√©es sur Spark"""
    info = spark_processor.get_spark_application_info()
    if info:
        return jsonify(info)
    else:
        return jsonify({'error': 'Pas de session Spark active'})

@app.route('/api/real-data')
def get_real_data():
    """Afficher TOUTES vos vraies donn√©es CSV"""
    try:
        if spark_processor.original_data:
            # R√©cup√©rer TOUTES vos donn√©es (pas seulement un √©chantillon)
            all_data = spark_processor.original_data.collect()
            result = [row.asDict() for row in all_data]
            return jsonify({
                'status': 'success',
                'total_records': streaming_results['real_csv_records'],
                'all_data': result,  # Toutes vos donn√©es
                'message': f'Voici vos {len(result)} vrais enregistrements'
            })
        else:
            return jsonify({'error': 'Pas de donn√©es Spark charg√©es'})
    except Exception as e:
        return jsonify({'error': str(e)})

if __name__ == '__main__':
    print("üöÄ D√©marrage Application Spark - ANALYSE DE VOS VRAIES DONN√âES UNIQUEMENT")
    print("‚ö° Connexion √† Spark Master: spark://namenode:7077")
    print("üìä Mode: Analyse statique de vos 10 lignes CSV r√©elles")
    print("üö´ Aucune g√©n√©ration de donn√©es al√©atoires")
    
    # D√©marrer Flask
    app.run(host='0.0.0.0', port=5000, debug=False, threaded=True)